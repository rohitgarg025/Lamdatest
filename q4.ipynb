{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.18.0.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow<=7.2.0 in /usr/lib/python3/dist-packages (from gym) (7.0.0)\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/rohit/.local/lib/python3.8/site-packages (from gym) (1.20.1)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/rohit/.local/lib/python3.8/site-packages (from gym) (1.5.4)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.18.0-py3-none-any.whl size=1656446 sha256=5ccdacc69affea81fbf11062c7418055347ac87bb40c30f10c1e4b2dc64aa8b3\n",
      "  Stored in directory: /home/rohit/.cache/pip/wheels/d8/e7/68/a3f0f1b5831c9321d7523f6fd4e0d3f83f2705a1cbd5daaa79\n",
      "Successfully built gym\n",
      "Installing collected packages: cloudpickle, pyglet, gym\n",
      "Successfully installed cloudpickle-1.6.0 gym-0.18.0 pyglet-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Solver starts\n",
      "---------------------------------\n",
      "Run: 1, exploration: 0.990025, score: 22\n",
      "Run: 2, exploration: 0.9322301194154049, score: 13\n",
      "Run: 3, exploration: 0.8778091417340573, score: 13\n",
      "Run: 4, exploration: 0.7744209942832988, score: 26\n",
      "Run: 5, exploration: 0.7292124703704616, score: 13\n",
      "Run: 6, exploration: 0.6797938283326578, score: 15\n",
      "Run: 7, exploration: 0.6433260027715241, score: 12\n",
      "Run: 8, exploration: 0.5937455908197752, score: 17\n",
      "Run: 9, exploration: 0.5732736268885887, score: 8\n",
      "Run: 10, exploration: 0.5425201222922789, score: 12\n",
      "Run: 11, exploration: 0.5134164023722473, score: 12\n",
      "Run: 12, exploration: 0.4858739637363176, score: 12\n",
      "Run: 13, exploration: 0.4484282034609769, score: 17\n",
      "Run: 14, exploration: 0.4222502236424958, score: 13\n",
      "Run: 15, exploration: 0.4036245882390106, score: 10\n",
      "Run: 16, exploration: 0.3877593341372176, score: 9\n",
      "Run: 17, exploration: 0.37251769488706843, score: 9\n",
      "Run: 18, exploration: 0.3472722151889232, score: 15\n",
      "Run: 19, exploration: 0.3017979588795719, score: 29\n",
      "Run: 20, exploration: 0.27853872940185365, score: 17\n",
      "Run: 21, exploration: 0.2662522686041882, score: 10\n",
      "Run: 22, exploration: 0.2532352299289372, score: 11\n",
      "Run: 23, exploration: 0.24328132378095624, score: 9\n",
      "Run: 24, exploration: 0.23489314109365644, score: 8\n",
      "Run: 25, exploration: 0.22566020663225933, score: 9\n",
      "Run: 26, exploration: 0.21248679717794605, score: 13\n",
      "Run: 27, exploration: 0.2041345879004775, score: 9\n",
      "Run: 28, exploration: 0.13066846301911936, score: 90\n",
      "Run: 29, exploration: 0.0996820918179746, score: 55\n",
      "Run: 30, exploration: 0.08239373898667031, score: 39\n",
      "Run: 31, exploration: 0.058010934765067, score: 71\n",
      "Run: 32, exploration: 0.046296535966244, score: 46\n",
      "Run: 33, exploration: 0.03585300941485119, score: 52\n",
      "Run: 34, exploration: 0.028186002814352063, score: 49\n",
      "Run: 35, exploration: 0.022835098131175, score: 43\n",
      "Run: 36, exploration: 0.018132788524664028, score: 47\n",
      "Run: 37, exploration: 0.013558238831322046, score: 59\n",
      "Run: 38, exploration: 0.010929385683282892, score: 44\n",
      "Run: 39, exploration: 0.01, score: 44\n",
      "Run: 40, exploration: 0.01, score: 54\n",
      "Run: 41, exploration: 0.01, score: 59\n",
      "Run: 42, exploration: 0.01, score: 72\n",
      "Run: 43, exploration: 0.01, score: 116\n",
      "Run: 44, exploration: 0.01, score: 118\n",
      "Run: 45, exploration: 0.01, score: 77\n",
      "Run: 46, exploration: 0.01, score: 80\n",
      "Run: 47, exploration: 0.01, score: 99\n",
      "Run: 48, exploration: 0.01, score: 61\n",
      "Run: 49, exploration: 0.01, score: 82\n",
      "Run: 50, exploration: 0.01, score: 71\n",
      "Run: 51, exploration: 0.01, score: 106\n",
      "Run: 52, exploration: 0.01, score: 273\n",
      "Run: 53, exploration: 0.01, score: 185\n",
      "Run: 54, exploration: 0.01, score: 500\n",
      "Run: 55, exploration: 0.01, score: 180\n",
      "Run: 56, exploration: 0.01, score: 175\n",
      "Run: 57, exploration: 0.01, score: 206\n",
      "Run: 58, exploration: 0.01, score: 204\n",
      "Run: 59, exploration: 0.01, score: 365\n",
      "Run: 60, exploration: 0.01, score: 241\n",
      "Run: 61, exploration: 0.01, score: 200\n",
      "Run: 62, exploration: 0.01, score: 181\n",
      "Run: 63, exploration: 0.01, score: 185\n",
      "Run: 64, exploration: 0.01, score: 233\n",
      "Run: 65, exploration: 0.01, score: 263\n",
      "Run: 66, exploration: 0.01, score: 293\n",
      "Run: 67, exploration: 0.01, score: 205\n",
      "Run: 68, exploration: 0.01, score: 212\n",
      "Run: 69, exploration: 0.01, score: 313\n",
      "Run: 70, exploration: 0.01, score: 190\n",
      "Run: 71, exploration: 0.01, score: 158\n",
      "Run: 72, exploration: 0.01, score: 156\n",
      "Run: 73, exploration: 0.01, score: 162\n",
      "Run: 74, exploration: 0.01, score: 137\n",
      "Run: 75, exploration: 0.01, score: 215\n",
      "Run: 76, exploration: 0.01, score: 306\n",
      "Run: 77, exploration: 0.01, score: 53\n",
      "Run: 78, exploration: 0.01, score: 160\n",
      "Run: 79, exploration: 0.01, score: 161\n",
      "Run: 80, exploration: 0.01, score: 150\n",
      "Run: 81, exploration: 0.01, score: 200\n",
      "Run: 82, exploration: 0.01, score: 82\n",
      "Run: 83, exploration: 0.01, score: 93\n",
      "Run: 84, exploration: 0.01, score: 149\n",
      "Run: 85, exploration: 0.01, score: 189\n",
      "Run: 86, exploration: 0.01, score: 260\n",
      "Run: 87, exploration: 0.01, score: 136\n",
      "Run: 88, exploration: 0.01, score: 179\n",
      "Run: 89, exploration: 0.01, score: 42\n",
      "Run: 90, exploration: 0.01, score: 80\n",
      "Run: 91, exploration: 0.01, score: 124\n",
      "Run: 92, exploration: 0.01, score: 57\n",
      "Run: 93, exploration: 0.01, score: 117\n",
      "Run: 94, exploration: 0.01, score: 128\n",
      "Run: 95, exploration: 0.01, score: 140\n",
      "Run: 96, exploration: 0.01, score: 259\n",
      "Run: 97, exploration: 0.01, score: 124\n",
      "Run: 98, exploration: 0.01, score: 121\n",
      "Run: 99, exploration: 0.01, score: 37\n",
      "Run: 100, exploration: 0.01, score: 118\n",
      "Run: 101, exploration: 0.01, score: 299\n",
      "Run: 102, exploration: 0.01, score: 500\n",
      "Run: 103, exploration: 0.01, score: 45\n",
      "Run: 104, exploration: 0.01, score: 13\n",
      "Run: 105, exploration: 0.01, score: 304\n",
      "Run: 106, exploration: 0.01, score: 97\n",
      "Run: 107, exploration: 0.01, score: 110\n",
      "Run: 108, exploration: 0.01, score: 112\n",
      "Run: 109, exploration: 0.01, score: 145\n",
      "Run: 110, exploration: 0.01, score: 137\n",
      "Run: 111, exploration: 0.01, score: 232\n",
      "Run: 112, exploration: 0.01, score: 138\n",
      "Run: 113, exploration: 0.01, score: 213\n",
      "Run: 114, exploration: 0.01, score: 336\n",
      "Run: 115, exploration: 0.01, score: 118\n",
      "Run: 116, exploration: 0.01, score: 500\n",
      "Run: 117, exploration: 0.01, score: 168\n",
      "Run: 118, exploration: 0.01, score: 269\n",
      "Run: 119, exploration: 0.01, score: 388\n",
      "Run: 120, exploration: 0.01, score: 129\n",
      "Run: 121, exploration: 0.01, score: 199\n",
      "Run: 122, exploration: 0.01, score: 125\n",
      "Run: 123, exploration: 0.01, score: 258\n",
      "Run: 124, exploration: 0.01, score: 239\n",
      "Run: 125, exploration: 0.01, score: 394\n",
      "Run: 126, exploration: 0.01, score: 215\n",
      "Run: 127, exploration: 0.01, score: 179\n",
      "Run: 128, exploration: 0.01, score: 97\n",
      "Run: 129, exploration: 0.01, score: 111\n",
      "Run: 130, exploration: 0.01, score: 297\n",
      "Run: 131, exploration: 0.01, score: 171\n",
      "Run: 132, exploration: 0.01, score: 151\n",
      "Run: 133, exploration: 0.01, score: 300\n",
      "Run: 134, exploration: 0.01, score: 140\n",
      "Run: 135, exploration: 0.01, score: 118\n",
      "Run: 136, exploration: 0.01, score: 353\n",
      "Run: 137, exploration: 0.01, score: 268\n",
      "Run: 138, exploration: 0.01, score: 182\n",
      "Run: 139, exploration: 0.01, score: 104\n",
      "Run: 140, exploration: 0.01, score: 168\n",
      "Run: 141, exploration: 0.01, score: 164\n",
      "Run: 142, exploration: 0.01, score: 303\n",
      "Run: 143, exploration: 0.01, score: 121\n",
      "Run: 144, exploration: 0.01, score: 165\n",
      "Run: 145, exploration: 0.01, score: 145\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "# from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from statistics import mean\n",
    "import h5py\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "MAX_MEMORY = 1000000\n",
    "BATCH_SIZE = 20\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_MIN = 0.01\n",
    "\n",
    "\n",
    "class ScoreEvaluator:\n",
    "\n",
    "    def __init__(self, max_len, average_of_last_runs, model = None):\n",
    "        self.max_len = max_len\n",
    "        self.score_table = deque(maxlen=self.max_len)\n",
    "        self.model = model\n",
    "        self.average_of_last_runs = average_of_last_runs\n",
    "\n",
    "    def store_score(self, episode, step):\n",
    "        self.score_table.append([episode, step])\n",
    "\n",
    "    def plot_evaluation(self, title = \"Training\"):\n",
    "        print(self.model.summary()) if self.model is not None else print(\"Model not defined!\")\n",
    "        avg_score = mean(self.score_table[1])\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(self.score_table)):\n",
    "            x.append(self.score_table[i][0])\n",
    "            y.append(self.score_table[i][1])\n",
    "\n",
    "        average_range = self.average_of_last_runs if self.average_of_last_runs is not None else len(x)\n",
    "        plt.plot(x, y, label=\"score per run\")\n",
    "        plt.plot(x[-average_range:], [np.mean(y[-average_range:])] * len(y[-average_range:]), linestyle=\"--\",\n",
    "                 label=\"last \" + str(average_range) + \" runs average\")\n",
    "        title = \"CartPole-v1 \" + str(title)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Runs\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.show()\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.exploration_rate = 1.0\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, input_shape=(observation_space,), activation='relu'))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        self.model.add(Dense(self.action_space, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(0, self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        else:\n",
    "            minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "            for state, action, reward, state_next, done in minibatch:\n",
    "                Q = reward\n",
    "                if not done:\n",
    "                    Q = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "                Q_values = self.model.predict(state)\n",
    "                Q_values[0][action] = Q\n",
    "                self.model.fit(state, Q_values, verbose=0)\n",
    "            self.exploration_rate *= EXPLORATION_DECAY\n",
    "            self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "class TrainSolver:\n",
    "\n",
    "    def __init__(self, max_episodes):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.score_table = deque(maxlen=400)\n",
    "        self.average_of_last_runs = None\n",
    "        self.model = None\n",
    "        self.play_episodes = 100\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        self.solver = Network(observation_space, action_space)\n",
    "\n",
    "    def train(self):\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"Solver starts\")\n",
    "        print(\"---------------------------------\")\n",
    "\n",
    "        self.model = self.solver.get_model()\n",
    "        episode = 0\n",
    "        score_eval = ScoreEvaluator(400, 50, self.model)\n",
    "        while episode < self.max_episodes:\n",
    "\n",
    "            episode += 1\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, observation_space])\n",
    "            step = 0\n",
    "            while True:\n",
    "\n",
    "                step += 1\n",
    "                # env.render()\n",
    "                action = self.solver.take_action(state)\n",
    "                state_next, reward, done, info = env.step(action)\n",
    "                if not done:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -reward\n",
    "                state_next = np.reshape(state_next, [1, observation_space])\n",
    "                self.solver.add_to_memory(state, action, reward, state_next, done)\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    print(\"Run: \" + str(episode) + \", exploration: \" + str(self.solver.exploration_rate) + \", score: \" + str(step))\n",
    "                    # self.score_table.append([episode, step])\n",
    "                    score_eval.store_score(episode, step)\n",
    "                    break\n",
    "                self.solver.experience_replay()\n",
    "        score_eval.plot_evaluation(\"Training\")\n",
    "\n",
    "    def return_trained_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def play(self, play_episodes=100, load_model=False, model_weights_dir=None, trained_model=None):\n",
    "\n",
    "        self.play_episodes = play_episodes\n",
    "        if load_model is not False:\n",
    "            if model_weights_dir is None:\n",
    "                print(\"Can't load specified model\")\n",
    "            elif trained_model is None:\n",
    "                print(\"Please pass a valid model as a parameter\")\n",
    "            else:\n",
    "                model = trained_model\n",
    "                model.load(model_weights_dir)\n",
    "        else:\n",
    "            model = self.model\n",
    "\n",
    "        env = gym.make('CartPole-v1')\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        episode = 0\n",
    "        score_eval = ScoreEvaluator(400, 100, model)\n",
    "        while episode < self.play_episodes:\n",
    "\n",
    "            episode += 1\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, observation_space])\n",
    "            step = 0\n",
    "            while True:\n",
    "\n",
    "                step += 1\n",
    "                env.render()\n",
    "                action = self.solver.take_action(state)\n",
    "                state_next, reward, done, info = env.step(action)\n",
    "\n",
    "                if not done:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -reward\n",
    "                state_next = np.reshape(state_next, [1, observation_space])\n",
    "                self.solver.add_to_memory(state, action, reward, state_next, done)\n",
    "                state = state_next\n",
    "\n",
    "                if done:\n",
    "                    print(\"Run: \" + str(episode) + \", score: \" + str(\n",
    "                        step))\n",
    "                    # self.score_table.append([episode, step])\n",
    "                    score_eval.store_score(episode, step)\n",
    "                    break\n",
    "                self.solver.experience_replay()\n",
    "        score_eval.plot_evaluation(\"100 Plays\")\n",
    "\n",
    "    def save_model(self):\n",
    "        self.model.save('cartpole_model.h5')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = TrainSolver(150)\n",
    "    trainer.train()\n",
    "    trainer.play(100)\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
